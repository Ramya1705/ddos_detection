{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3ff38dbf-28a6-468c-ae03-02e28b9e8f92",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from typing import List, Optional, Dict, Tuple\n",
    "from collections import deque, defaultdict\n",
    "from joblib import Parallel, delayed\n",
    "import logging\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "\n",
    "DEFAULT_VOLUME_FEATURES = [\n",
    "    'Total Fwd Packets', 'Total Backward Packets',\n",
    "    'Total Length of Fwd Packets', 'Total Length of Bwd Packets',\n",
    "    'Fwd Packet Length Max', 'Fwd Packet Length Min', 'Fwd Packet Length Mean', 'Fwd Packet Length Std',\n",
    "    'Bwd Packet Length Max', 'Bwd Packet Length Min', 'Bwd Packet Length Mean', 'Bwd Packet Length Std',\n",
    "    'Fwd PSH Flags', 'Bwd PSH Flags', 'Fwd URG Flags', 'Bwd URG Flags',\n",
    "    'Fwd Header Length', 'Bwd Header Length',\n",
    "    'Min Packet Length', 'Max Packet Length', 'Packet Length Mean', 'Packet Length Std', 'Packet Length Variance',\n",
    "    'Down/Up Ratio', 'Average Packet Size', 'Avg Fwd Segment Size', 'Avg Bwd Segment Size',\n",
    "    'Fwd Avg Bytes/Bulk', 'Fwd Avg Packets/Bulk',\n",
    "    'Bwd Avg Bytes/Bulk', 'Bwd Avg Packets/Bulk',\n",
    "    'Subflow Fwd Packets', 'Subflow Fwd Bytes', 'Subflow Bwd Packets', 'Subflow Bwd Bytes',\n",
    "    'Init_Win_bytes_forward', 'Init_Win_bytes_backward',\n",
    "    'act_data_pkt_fwd', 'min_seg_size_forward',\n",
    "    'Flow Bytes/s', 'Flow Packets/s', 'Fwd Packets/s', 'Bwd Packets/s',\n",
    "    'Fwd Avg Bulk Rate', 'Bwd Avg Bulk Rate'\n",
    "]\n",
    "\n",
    "DEFAULT_TEMPORAL_FEATURES = [\n",
    "    'Flow Duration',\n",
    "    'Flow IAT Mean', 'Flow IAT Std', 'Flow IAT Max', 'Flow IAT Min',\n",
    "    'Fwd IAT Total', 'Fwd IAT Mean', 'Fwd IAT Std', 'Fwd IAT Max', 'Fwd IAT Min',\n",
    "    'Bwd IAT Total', 'Bwd IAT Mean', 'Bwd IAT Std', 'Bwd IAT Max', 'Bwd IAT Min',\n",
    "    'Active Mean', 'Active Std', 'Active Max', 'Active Min',\n",
    "    'Idle Mean', 'Idle Std', 'Idle Max', 'Idle Min',\n",
    "    'FIN Flag Count', 'SYN Flag Count', 'RST Flag Count', 'PSH Flag Count', 'ACK Flag Count',\n",
    "    'URG Flag Count', 'CWE Flag Count', 'ECE Flag Count',\n",
    "]\n",
    "\n",
    "DEFAULT_ENTROPY_FEATURES = [\n",
    "    'Source IP', 'Source Port', 'Destination IP', 'Destination Port', 'Protocol'\n",
    "]\n",
    "\n",
    "class RollingStats:\n",
    "    def __init__(self, window: int):\n",
    "        self.window = window\n",
    "        self.values = deque()\n",
    "\n",
    "    def update(self, value: float):\n",
    "        self.values.append(value)\n",
    "        if len(self.values) > self.window:\n",
    "            self.values.popleft()\n",
    "\n",
    "    def compute(self) -> Tuple[float, float, float, float, float, float]:\n",
    "        arr = np.array(self.values)\n",
    "        if len(arr) == 0:\n",
    "            return 0, 1e-9, 0, 1e-9, 0, 0\n",
    "        mean = np.mean(arr)\n",
    "        std = np.std(arr) or 1e-9\n",
    "        med = np.median(arr)\n",
    "        mad = np.median(np.abs(arr - med)) or 1e-9\n",
    "        q1 = np.percentile(arr, 25)\n",
    "        q3 = np.percentile(arr, 75)\n",
    "        return mean, std, med, mad, q1, q3\n",
    "\n",
    "class EntropyTracker:\n",
    "    def __init__(self, window: int):\n",
    "        self.window = window\n",
    "        self.history = deque(maxlen=window)\n",
    "        self.value_counts = defaultdict(int)\n",
    "        self.total = 0\n",
    "        \n",
    "    def update(self, value: str):\n",
    "        if len(self.history) == self.window:\n",
    "            old_val = self.history[0]\n",
    "            self.value_counts[old_val] -= 1\n",
    "            if self.value_counts[old_val] <= 0:\n",
    "                del self.value_counts[old_val]\n",
    "            self.total -= 1\n",
    "            \n",
    "        self.history.append(value)\n",
    "        self.value_counts[value] += 1\n",
    "        self.total += 1\n",
    "        \n",
    "    def compute_entropy(self) -> float:\n",
    "        if self.total == 0:\n",
    "            return 0.0\n",
    "            \n",
    "        entropy = 0.0\n",
    "        for count in self.value_counts.values():\n",
    "            probability = count / self.total\n",
    "            entropy -= probability * math.log(probability, 2)\n",
    "            \n",
    "        max_entropy = math.log(len(self.value_counts), 2) if len(self.value_counts) > 0 else 1.0\n",
    "        normalized_entropy = entropy / max_entropy if max_entropy > 0 else 0.0\n",
    "        \n",
    "        return normalized_entropy\n",
    "\n",
    "class AnomalyScorer:\n",
    "    def __init__(self,\n",
    "                 window: int = 100,\n",
    "                 volume_features: Optional[List[str]] = None,\n",
    "                 temporal_features: Optional[List[str]] = None,\n",
    "                 entropy_features: Optional[List[str]] = None,\n",
    "                 keep_details: bool = False,\n",
    "                 log_level: str = 'INFO',\n",
    "                 n_jobs: int = -1):\n",
    "        self.window = window\n",
    "        self.volume_features = volume_features or DEFAULT_VOLUME_FEATURES\n",
    "        self.temporal_features = temporal_features or DEFAULT_TEMPORAL_FEATURES\n",
    "        self.entropy_features = entropy_features or DEFAULT_ENTROPY_FEATURES\n",
    "        self.keep_details = keep_details\n",
    "        self.n_jobs = n_jobs\n",
    "        self._setup_logging(log_level)\n",
    "        self.stats = {}\n",
    "        self.entropy_trackers = {}\n",
    "        self.top_contributors = {}\n",
    "\n",
    "    def _setup_logging(self, log_level: str):\n",
    "        self.logger = logging.getLogger('AnomalyScorer')\n",
    "        self.logger.setLevel(log_level)\n",
    "        if not self.logger.handlers:\n",
    "            ch = logging.StreamHandler()\n",
    "            formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')\n",
    "            ch.setFormatter(formatter)\n",
    "            self.logger.addHandler(ch)\n",
    "\n",
    "    def _init_stats(self, features: List[str]):\n",
    "        for f in features:\n",
    "            if f not in self.stats:\n",
    "                self.stats[f] = RollingStats(self.window)\n",
    "\n",
    "    def _init_entropy_trackers(self, features: List[str]):\n",
    "        for f in features:\n",
    "            if f not in self.entropy_trackers:\n",
    "                self.entropy_trackers[f] = EntropyTracker(self.window)\n",
    "\n",
    "    def _compute_feature_scores(self, df: pd.DataFrame, features: List[str], directional: bool = False) -> Dict[str, pd.Series]:\n",
    "        self._init_stats(features)\n",
    "        scores = {f: [] for f in features}\n",
    "    \n",
    "        for i in tqdm(range(len(df)), desc=\"Scoring features\"):\n",
    "            for f in features:\n",
    "                val = df.iloc[i][f]\n",
    "                self.stats[f].update(val)\n",
    "                mean, std, med, mad, q1, q3 = self.stats[f].compute()\n",
    "    \n",
    "                if directional:\n",
    "                    z = max(0, min(1, (val - mean) / (3 * std) + 0.5))  # Scaled to 0-1 range\n",
    "                    mad_score = max(0, min(1, (val - med) / (3 * mad) + 0.5))\n",
    "                else:\n",
    "                    z = min(1, abs((val - mean) / (3 * std)))  # Capped at 1 (3 sigma)\n",
    "                    mad_score = min(1, abs((val - med) / (3 * mad)))\n",
    "    \n",
    "                iqr_outlier = float(val < q1 - 1.5 * (q3 - q1) or val > q3 + 1.5 * (q3 - q1))\n",
    "                score = 0.3 * z + 0.3 * mad_score + 0.4 * iqr_outlier\n",
    "                scores[f].append(min(1, max(0, score)))  # Ensure score is between 0-1\n",
    "    \n",
    "        return {f: pd.Series(scores[f], index=df.index) for f in features}\n",
    "\n",
    "    def _compute_entropy_scores(self, df: pd.DataFrame, features: List[str]) -> Dict[str, pd.Series]:\n",
    "        self._init_entropy_trackers(features)\n",
    "        scores = {f: [] for f in features}\n",
    "        \n",
    "        for i in tqdm(range(len(df)), desc=\"Computing entropy scores\"):\n",
    "            for f in features:\n",
    "                val = str(df.iloc[i][f])\n",
    "                self.entropy_trackers[f].update(val)\n",
    "                entropy = self.entropy_trackers[f].compute_entropy()\n",
    "                \n",
    "                # Low entropy is more anomalous (common values dominate)\n",
    "                entropy_score = 1.0 - entropy\n",
    "                scores[f].append(min(1, max(0, entropy_score)))\n",
    "                \n",
    "        return {f: pd.Series(scores[f], index=df.index) for f in features}\n",
    "\n",
    "    def compute_scores(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        start_time = time.time()\n",
    "        self.logger.info(\"Starting anomaly score computation...\")\n",
    "\n",
    "        df_proc = df.copy()\n",
    "        vol_feats = [f for f in self.volume_features if f in df_proc.columns]\n",
    "        temp_feats = [f for f in self.temporal_features if f in df_proc.columns]\n",
    "        entropy_feats = [f for f in self.entropy_features if f in df_proc.columns]\n",
    "\n",
    "        # Compute all scores\n",
    "        vol_scores = self._compute_feature_scores(df_proc, vol_feats, directional=True)\n",
    "        temp_scores = self._compute_feature_scores(df_proc, temp_feats, directional=False)\n",
    "        entropy_scores = self._compute_entropy_scores(df_proc, entropy_feats)\n",
    "\n",
    "        if self.keep_details:\n",
    "            # Store individual feature scores if requested\n",
    "            for f, s in {**vol_scores, **temp_scores, **entropy_scores}.items():\n",
    "                df_proc[f'AS_{f}'] = s\n",
    "\n",
    "        # Combine scores using standard deviation (normalized to 0-1 range)\n",
    "        df_proc['VOLUME_ANOMALY_SCORE'] = 0.7 * pd.DataFrame(vol_scores).sum(axis=1) + 0.3 * pd.DataFrame(vol_scores).max(axis=1)\n",
    "        df_proc['TEMPORAL_ANOMALY_SCORE'] = 0.7 * pd.DataFrame(temp_scores).sum(axis=1) + 0.3 * pd.DataFrame(temp_scores).max(axis=1)\n",
    "        df_proc['ENTROPY_ANOMALY_SCORE'] = 0.7 * pd.DataFrame(entropy_scores).sum(axis=1) + 0.3 * pd.DataFrame(entropy_scores).max(axis=1)\n",
    "\n",
    "        # Store top contributors for each category\n",
    "        self._store_top_contributors(vol_scores, 'VOLUME')\n",
    "        self._store_top_contributors(temp_scores, 'TEMPORAL')\n",
    "        self._store_top_contributors(entropy_scores, 'ENTROPY')\n",
    "\n",
    "        elapsed = time.time() - start_time\n",
    "        self.logger.info(f\"Anomaly scoring completed in {elapsed:.2f} seconds\")\n",
    "\n",
    "        return df_proc\n",
    "\n",
    "    def _store_top_contributors(self, scores: Dict[str, pd.Series], label: str):\n",
    "        score_df = pd.DataFrame(scores)\n",
    "        self.top_contributors[f'{label}_TOP3'] = score_df.apply(\n",
    "            lambda row: row.nlargest(3).index.tolist(), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec606251-8f46-4515-bdaf-fbd641140cae",
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_path = \"input_data11.csv\"       # edit path\n",
    "df   = pd.read_csv(csv_path,low_memory=False)\n",
    "df= df[:10000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "628a3e15-2588-4286-ba16-456d0fe94aff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df1\n",
    "# import ipaddress\n",
    "\n",
    "# # Convert IPv4 address to int\n",
    "# df1[' Source IP'] = df[' Source IP'].apply(lambda x: int(ipaddress.IPv4Address(x)))\n",
    "# df1[' Destination IP'] = df[' Destination IP'].apply(lambda x: int(ipaddress.IPv4Address(x)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92830237-0f55-4722-95ba-1d96a9d21af0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- load your CIC Flow-Meter CSV --------------------------------\n",
    "# csv_path = \"datasets/UDPLag.csv/UDPLag.csv\"       # edit path\n",
    "# df   = pd.read_csv(csv_path)\n",
    "# df=df[:4000]\n",
    "df=df\n",
    "df.columns =df.columns.str.strip()\n",
    "df.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "\n",
    "# Replace NaNs with column mean or 0\n",
    "df.fillna(df.mean(), inplace=True)\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "# scaler = StandardScaler()\n",
    "# # flow_features= df.drop(columns=[\"VOLUME_ANOMALY_SCORE\",\"TEMPORAL_ANOMALY_SCORE\",\"ENTROPY_ANOMALY_SCORE\"])\n",
    "# flow_features.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "\n",
    "# # Replace NaNs with column mean or 0\n",
    "# flow_features.fillna(flow_features.mean(), inplace=True)\n",
    "\n",
    "# # Fit the scaler on training data and transform both train and test\n",
    "# data = scaler.fit_transform(flow_features)\n",
    "# df2 = pd.DataFrame(data,columns=flow_features.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d91dd318-1afd-4068-8014-7e840ed97036",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Initialize with parallel processing (uses all cores)\n",
    "scorer = AnomalyScorer(\n",
    "    window=1000,\n",
    "    keep_details=False,\n",
    "    \n",
    "    log_level='INFO',\n",
    "    n_jobs=1\n",
    ")\n",
    "\n",
    "# Load your data\n",
    "# df_raw = pd.read_csv('your_data.csv')\n",
    "\n",
    "# Process data\n",
    "result_df= scorer.compute_scores(df)\n",
    "# Access results\n",
    "# normalized_volume = result_df['NORM_VOLUME_SCORE']\n",
    "# normalized_temporal = result_df['NORM_TEMPORAL_SCORE']\n",
    "# overall_score = result_df['OVERALL_SCORE']\n",
    "\n",
    "# # Get top contributing features\n",
    "# print(scorer.top_contributors['VOLUME_TOP3'].tail())  # Recent volume contributors\n",
    "# print(scorer.top_contributors['TEMPORAL_TOP3'].tail()) # Recent temporal contributors\n",
    "\n",
    "# # Access top contributors\n",
    "# print(\"Top volume contributors:\", scorer.top_contributors['VOLUME_TOP3'].head())\n",
    "# print(\"Top temporal contributors:\", scorer.top_contributors['TEMPORAL_TOP3'].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b1f64e0-b976-46e9-83c2-f35d1880cda7",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df.to_csv(\"test_df11.csv\",index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
